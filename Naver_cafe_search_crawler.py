#!/usr/bin/env python
# coding: utf-8

import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchWindowException, WebDriverException
import pandas as pd
from bs4 import BeautifulSoup as bs
import os
from urllib.request import urlretrieve

# DataFrame to store results
df = pd.DataFrame([], columns=["title", "post_number", "date", "author_id", "author_nickname",
                              "article_content_html", "html_file_path", "commenter_ids",
                              "commenter_nicknames", "comments", "comment_dates"])

# Setup Chrome driver
options = webdriver.ChromeOptions()
options.add_experimental_option("excludeSwitches", ["enable-logging"])
options.add_argument("--disable-blink-features=AutomationControlled")
options.add_experimental_option("useAutomationExtension", False)
driver = webdriver.Chrome(options=options)
driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

# Navigate to Naver login page
driver.get('https://nid.naver.com/nidlogin.login')

# Target search URL - ê²Œì„ì¡°ì•„ ì‘ì„±ìë¡œ ê²€ìƒ‰ëœ í˜ì´ì§€
target_url_template = 'https://cafe.naver.com/f-e/cafes/12323151/menus/0?q=%EA%B2%8C%EC%9E%84%EC%A1%B0%EC%95%84&ta=WRITER&page={}'

print("="*50)
print("Naver Cafe Search Result Crawler")
print("Target: ê²Œì„ì¡°ì•„ ì‘ì„±ìì˜ ê²Œì‹œê¸€")
print("="*50)

# Manual login
print("\n1. ë„¤ì´ë²„ ì¹´í˜ì— ìˆ˜ë™ìœ¼ë¡œ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
print("2. ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
input("\nPress Enter after login: ")

# Settings
crawl_all_pages = input("ëª¨ë“  í˜ì´ì§€ë¥¼ ìë™ìœ¼ë¡œ í¬ë¡¤ë§í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: y): ").lower()
if crawl_all_pages != 'n':
    num_pages = float('inf')  # ë¬´ì œí•œìœ¼ë¡œ ì„¤ì •
    print("\nëª¨ë“  í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤. (ìë™ìœ¼ë¡œ ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€)")
else:
    num_pages = int(input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 3): ") or "3")
    print(f"\n{num_pages}í˜ì´ì§€ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

save_html_files = input("HTMLì„ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: y): ").lower()
save_html_files = save_html_files != 'n'  # ê¸°ë³¸ê°’ì„ yë¡œ ì„¤ì •

if save_html_files and not os.path.exists("saved_html"):
    os.makedirs("saved_html")

index = 0
page = 1

def check_driver_alive(driver):
    """ë¸Œë¼ìš°ì €ê°€ ì‚´ì•„ìˆëŠ”ì§€ í™•ì¸"""
    try:
        driver.current_url
        return True
    except (NoSuchWindowException, WebDriverException):
        return False

while page <= num_pages:
    if num_pages == float('inf'):
        print(f"\n=== Page {page} ì²˜ë¦¬ ì¤‘ ===")
    else:
        print(f"\n=== Page {page}/{num_pages} ì²˜ë¦¬ ì¤‘ ===")

    # ë¸Œë¼ìš°ì € ìƒíƒœ í™•ì¸
    if not check_driver_alive(driver):
        print("âŒ ë¸Œë¼ìš°ì € ì°½ì´ ë‹«í˜”ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        print("ğŸ’¡ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ê³  ë¸Œë¼ìš°ì €ë¥¼ ì—´ì–´ë‘” ìƒíƒœë¡œ ì§„í–‰í•´ì£¼ì„¸ìš”.")
        break

    try:
        current_url = target_url_template.format(page)
        driver.get(current_url)
        time.sleep(3)
    except (NoSuchWindowException, WebDriverException) as e:
        print(f"âŒ Page {page} ë¡œë”© ì‹¤íŒ¨: ë¸Œë¼ìš°ì € ì—°ê²° ì˜¤ë¥˜")
        print("ğŸ’¡ ë¸Œë¼ìš°ì €ê°€ ë‹«í˜”ê±°ë‚˜ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")
        break

    # Wait for page to load
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
    except:
        print(f"Page {page} loading failed, skipping...")
        page += 1
        continue

    # Parse page source
    soup = bs(driver.page_source, 'html.parser')

    # Find article links in search results
    article_links = []

    # Method 1: Look for article links in search results
    for link in soup.find_all('a', href=True):
        href = link.get('href')
        if href and ('/articles/' in href or 'articleid=' in href):
            if href.startswith('/'):
                article_links.append('https://cafe.naver.com' + href)
            elif not href.startswith('http'):
                article_links.append('https://cafe.naver.com/' + href)
            else:
                article_links.append(href)

    # Remove duplicates
    article_links = list(set(article_links))

    print(f"Found {len(article_links)} articles on page {page}")

    # ê²Œì‹œê¸€ì´ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ íŒë‹¨í•˜ê³  ì¢…ë£Œ
    if not article_links:
        print(f"âŒ Page {page}ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ íŒë‹¨í•˜ê³  í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    # Process each article
    for i, article_url in enumerate(article_links[:10]):  # Limit to 10 articles per page
        print(f"  Processing article {i+1}/{min(len(article_links), 10)}: {article_url[:50]}...")

        # ë¸Œë¼ìš°ì € ìƒíƒœ í™•ì¸
        if not check_driver_alive(driver):
            print("âŒ ë¸Œë¼ìš°ì € ì°½ì´ ë‹«í˜”ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

        try:
            driver.get(article_url)
            time.sleep(2)
        except (NoSuchWindowException, WebDriverException) as e:
            print(f"    âŒ ê²Œì‹œê¸€ ë¡œë”© ì‹¤íŒ¨: {str(e)[:50]}...")
            continue

        # Try to switch to cafe_main frame if exists
        try:
            driver.switch_to.frame("cafe_main")
        except:
            pass

        # Extract article information - wrap entire processing in try-except
        try:
            # Title
            try:
                title_elem = driver.find_element(By.CSS_SELECTOR, ".title_text, .ArticleTitle, .post-title, h3, h2")
                title = title_elem.text.strip()
            except:
                title = "Title not found"

            # Date
            try:
                date_elem = driver.find_element(By.CSS_SELECTOR, ".date, .ArticleDate, .post-date, .created-date")
                date = date_elem.text.strip()
            except:
                date = "Date not found"

            # Author nickname
            try:
                author_elem = driver.find_element(By.CSS_SELECTOR, ".nickname, .ArticleWriter, .author, .writer")
                author_nickname = author_elem.text.strip()
            except:
                author_nickname = "Author not found"

            # Author ID (from profile link)
            try:
                author_link = driver.find_element(By.CSS_SELECTOR, ".thumb, .profile-link")
                author_href = author_link.get_attribute("href")
                author_id = ""
                if author_href and "members/" in author_href:
                    author_id = author_href.split("members/")[-1]
            except:
                author_id = "ID not found"

            # Extract ArticleContentBox inner HTML (í•˜ìœ„ HTMLë§Œ ì¶”ì¶œ)
            try:
                content_elem = driver.find_element(By.CSS_SELECTOR, "#app > div > div > div.ArticleContentBox")
                article_content_html = content_elem.get_attribute("innerHTML")
            except:
                try:
                    # Fallback to other content containers
                    content_elem = driver.find_element(By.CSS_SELECTOR, ".ArticleContentBox")
                    article_content_html = content_elem.get_attribute("innerHTML")
                except:
                    try:
                        content_elem = driver.find_element(By.CSS_SELECTOR, ".se-main-container, .post-content, .content")
                        article_content_html = content_elem.get_attribute("innerHTML")
                    except:
                        article_content_html = "Content HTML not found"

            # Extract post number from URL first (needed for filename)
            post_number = "Unknown"
            if "articleid=" in article_url:
                post_number = article_url.split("articleid=")[-1].split("&")[0]
            elif "articles/" in article_url:
                post_number = article_url.split("articles/")[-1].split("?")[0]

            # Save HTML to individual file
            html_file_path = ""
            if save_html_files and article_content_html != "Content HTML not found":
                try:
                    # íŠ¹ì • CSS selectorì—ì„œ ì œëª© ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©
                    try:
                        title_elem_for_filename = driver.find_element(By.CSS_SELECTOR, "#app > div > div > div.ArticleContentBox > div.article_header > div:nth-child(1) > div > div > h3")
                        title_for_filename = title_elem_for_filename.text.strip()
                    except:
                        # Fallback: ê¸°ì¡´ title ì‚¬ìš©
                        title_for_filename = title.split('\n')[0].strip()

                    safe_title = "".join(c for c in title_for_filename if c.isalnum() or c in (' ', '-', '_')).rstrip()
                    safe_title = safe_title.replace(' ', '_')[:50]  # ìµœëŒ€ 50ìë¡œ ì œí•œ
                    if not safe_title:  # ì œëª©ì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ëª… ì‚¬ìš©
                        safe_title = f"article_{post_number}"

                    html_filename = f"{safe_title}.html"
                    html_file_path = f"saved_html/{html_filename}"

                    with open(html_file_path, 'w', encoding='utf-8') as f:
                        f.write(f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{title_for_filename}</title>
</head>
<body>
    <h1>{title}</h1>
    <p><strong>ì‘ì„±ì:</strong> {author_nickname} ({author_id})</p>
    <p><strong>ì‘ì„±ì¼:</strong> {date}</p>
    <p><strong>ê²Œì‹œê¸€ ë²ˆí˜¸:</strong> {post_number}</p>
    <hr>
    {article_content_html}
</body>
</html>""")
                    print(f"    HTML saved: {html_filename}")
                except Exception as e:
                    print(f"    HTML save failed: {str(e)[:30]}...")
                    html_file_path = ""

            # Comments (simplified)
            commenter_ids = []
            commenter_nicknames = []
            comments = []
            comment_dates = []

            try:
                comment_elements = driver.find_elements(By.CSS_SELECTOR, ".comment_area, .CommentItem")[:5]  # Limit to 5 comments
                for comment_elem in comment_elements:
                    try:
                        comment_text = comment_elem.find_element(By.CSS_SELECTOR, ".text_comment, .comment-text").text.strip()
                        comment_author = comment_elem.find_element(By.CSS_SELECTOR, ".comment_nickname, .comment-author").text.strip()

                        comments.append(comment_text)
                        commenter_nicknames.append(comment_author)
                        commenter_ids.append("ID_not_extracted")
                        comment_dates.append("Date_not_extracted")
                    except:
                        continue
            except:
                pass

            if not comments:
                comments = ["No comments"]
                commenter_nicknames = ["No comments"]
                commenter_ids = ["No comments"]
                comment_dates = ["No comments"]

            # Add to dataframe
            df.loc[index] = [
                title, post_number, date, author_id, author_nickname, article_content_html,
                html_file_path, "; ".join(commenter_ids), "; ".join(commenter_nicknames),
                "; ".join(comments), "; ".join(comment_dates)
            ]

            # Save after each article
            df.to_csv("naver_cafe_crawling_results.csv", encoding="utf-8-sig", index=False)

            print(f"    âœ“ {title[:30]}...")
            index += 1

        except Exception as e:
            print(f"    âœ— Error processing article: {str(e)[:50]}...")
            continue

        # Switch back to default content for next iteration
        driver.switch_to.default_content()

    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
    page += 1

print(f"\n=== í¬ë¡¤ë§ ì™„ë£Œ ===")
print(f"ì´ {len(df)}ê°œì˜ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
print(f"ê²°ê³¼ëŠ” 'naver_cafe_crawling_results.csv' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# Display results summary
print("\n=== ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ===")
print(df[['title', 'author_nickname', 'date']].head(10))

driver.quit()